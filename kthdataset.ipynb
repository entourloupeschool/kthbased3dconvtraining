{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This database contains sequences of six classes of actions performed by\n",
    "# 25 subjects in four different conditions d1-d4\n",
    "\n",
    "# d1 - Static homogenous background\n",
    "# d2 - -\"-                          + Scale variations\n",
    "# d3 - -\"-                          + Different clothes\n",
    "# d4 - -\"-                          + Lighting variations\n",
    "\n",
    "# Training:   person11, 12, 13, 14, 15, 16, 17, 18\n",
    "# Validation: person19, 20, 21, 23, 24, 25, 01, 04\n",
    "# Test:       person22, 02, 03, 05, 06, 07, 08, 09, 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This machine has 8 CPU cores.\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import av\n",
    "import random as rdm\n",
    "import torchvision.transforms.v2 as transforms\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_video\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import multiprocessing\n",
    "import time\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from utils import *\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "print(\"This machine has {} CPU cores.\".format(num_cores))\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    rdm.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    return seed\n",
    "\n",
    "set_seed()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training are persons :11, 12, 13, 14, 15, 16, 17, 18\n",
    "# validation are persons : 19, 20, 21, 23, 24, 25, 01, 04\n",
    "# test are persons : 22, 02, 03, 05, 06, 07, 08, 09, 10\n",
    "# https://pytorch.org/vision/main/auto_examples/plot_optical_flow.html#sphx-glr-auto-examples-plot-optical-flow-py\n",
    "\n",
    "class KTHDataset(Dataset):\n",
    "    def __init__(self, root, txt_path, transforms=None, subset='train'):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.subset = subset\n",
    "        self.data = []\n",
    "        self.label_to_int = {\"boxing\": 0, \"handclapping\": 1, \"handwaving\": 2,\n",
    "                             \"jogging\": 3, \"running\": 4, \"walking\": 5}\n",
    "        self.int_to_label = {v: k for k, v in self.label_to_int.items()}\n",
    "\n",
    "        with open(txt_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                video_id, *frame_sequences = line.strip().split(',')\n",
    "                person_id, action, _ = video_id.split('_')\n",
    "\n",
    "                person_number = int(person_id.replace(\"person\", \"\"))\n",
    "                \n",
    "                if self._is_in_subset(person_number):\n",
    "                    for frame_sequence in frame_sequences:\n",
    "                        start, end = frame_sequence.split('-')\n",
    "                        self.data.append((video_id, int(start), int(end), self.label_to_int[action]))\n",
    "\n",
    "    def get_action(self, label):\n",
    "        return self.int_to_label[label]\n",
    "    \n",
    "    def _is_in_subset(self, person_number):\n",
    "        # check if the person number is in the subset\n",
    "        if self.subset == 'train':\n",
    "            return person_number in [11, 12, 13, 14, 15, 16, 17, 18]\n",
    "        elif self.subset == 'validation':\n",
    "            return person_number in [19, 20, 21, 23, 24, 25, 1, 4]\n",
    "        elif self.subset == 'test':\n",
    "            return person_number in [22, 2, 3, 5, 6, 7, 8, 9, 10]\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_id, start_frame, end_frame, action = self.data[idx]\n",
    "        video_path = os.path.join(self.root+'/'+self.get_action(action)+'/', video_id+'_uncomp' + '.avi') \n",
    "        video, audio, info = read_video(video_path, start_frame, end_frame, output_format=\"TCHW\")\n",
    "\n",
    "        if self.transforms:\n",
    "            video = self.transforms(video)\n",
    "\n",
    "        return video, action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/KTH'\n",
    "sequences_path = './data/KTH/sequences.txt'\n",
    "\n",
    "batch_size = 4\n",
    "resized = (64, 64)\n",
    "\n",
    "transform_outer = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "])\n",
    "\n",
    "transform_inner = transforms.Compose([\n",
    "    transforms.Resize(resized, antialias=True),\n",
    "    transforms.ToImageTensor(),\n",
    "    transforms.ConvertImageDtype(torch.float32),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transform_outer,\n",
    "    transform_inner\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transform_inner\n",
    "])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    videos, labels = zip(*batch)\n",
    "    # Pad videos in the batch\n",
    "    videos = pad_sequence([torch.from_numpy(np.array(vid)) for vid in videos], batch_first=True)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return videos, labels\n",
    "\n",
    "\n",
    "def get_loaders(data_path, sequences_path, batch_size, transform_train, transform_test, num_wokers=2, pin_memory=True, collate_fn=collate_fn):\n",
    "    train_dataset = KTHDataset(data_path, sequences_path, transforms=transform_train, subset='train')\n",
    "    val_dataset = KTHDataset(data_path, sequences_path, transforms=transform_test, subset='validation')\n",
    "    test_dataset = KTHDataset(data_path, sequences_path, transforms=transform_test, subset='test')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_wokers, pin_memory=pin_memory, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_wokers, pin_memory=pin_memory, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_wokers, pin_memory=pin_memory, collate_fn=collate_fn)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/in_vite/Library/Python/3.11/lib/python/site-packages/torchvision/io/video.py:161: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of batch : torch.Size([4, 137, 3, 64, 64])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# Test the dataloader\n",
    "train_set, _, _ = get_loaders(data_path, sequences_path, batch_size, transform_train, transform_test, num_wokers=0, pin_memory=True, collate_fn=collate_fn)\n",
    "for i, (videos, labels) in enumerate(train_set):\n",
    "    print('shape of batch :', videos.shape) # should get [batch_size, seq_len, 3, resized[0], resized[1]]\n",
    "    print(labels.shape) # should get [batch_size], the labels of the videos in the batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Simple3DCNN(nn.Module):\n",
    "    def __init__(self, num_classes=6, in_channels=3, dropout_prob=0.5):\n",
    "        super(Simple3DCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv3d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 32 * 32, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)  # Assume 6 classes for output\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)  # Dropout layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Conv1 + ReLU + MaxPool\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool3d(x, 2, 2)\n",
    "        \n",
    "        # Conv2 + ReLU + MaxPool\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool3d(x, 2, 2)\n",
    "        \n",
    "        # Flatten the tensor\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # FC1 + ReLU + Dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # FC2\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric: \n",
    "  def __init__(self):\n",
    "    self.loss_train = []\n",
    "    self.loss_test = []\n",
    "    self.acc_train = []\n",
    "    self.acc_test = []\n",
    "    \n",
    "def epoch(data, model, criterion, optimizer=None, cuda=False):\n",
    "    \"\"\"\n",
    "    Make a pass (called epoch in English) on the data `data` with the\n",
    "     model `model`. Evaluates `criterion` as loss.\n",
    "     If `optimizer` is given, perform a training epoch using\n",
    "     the given optimizer, otherwise, perform an evaluation epoch (no backward)\n",
    "     of the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # indicates whether the model is in eval or train mode (some layers behave differently in train and eval)\n",
    "    model.eval() if optimizer is None else model.train()\n",
    "\n",
    "    # objects to store metric averages\n",
    "    avg_loss = AverageMeter()\n",
    "    avg_top1_acc = AverageMeter()\n",
    "    avg_top5_acc = AverageMeter()\n",
    "    avg_batch_time = AverageMeter()\n",
    "    global loss_plot\n",
    "\n",
    "    # we iterate on the batches\n",
    "    tic = time.time()\n",
    "    for i, (input, target) in enumerate(data):\n",
    "\n",
    "        if cuda: # only with GPU, and not with CPU\n",
    "            input = input.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "        # forward\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # backward if we are training\n",
    "        if optimizer:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # compute metrics\n",
    "        prec1, prec5 = accuracy(output, target, topk=(1, 5))\n",
    "        batch_time = time.time() - tic\n",
    "        tic = time.time()\n",
    "\n",
    "        # update\n",
    "        avg_loss.update(loss.item())\n",
    "        avg_top1_acc.update(prec1.item())\n",
    "        avg_top5_acc.update(prec5.item())\n",
    "        avg_batch_time.update(batch_time)\n",
    "        if optimizer:\n",
    "            loss_plot.update(avg_loss.val)\n",
    "        # print info\n",
    "        #if i % PRINT_INTERVAL == 0:\n",
    "        #    print('[{0:s} Batch {1:03d}/{2:03d}]\\t'\n",
    "        #          'Time {batch_time.val:.3f}s ({batch_time.avg:.3f}s)\\t'\n",
    "        #          'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "        #          'Prec@1 {top1.val:5.1f} ({top1.avg:5.1f})\\t'\n",
    "        #          'Prec@5 {top5.val:5.1f} ({top5.avg:5.1f})'.format(\n",
    "        #           \"EVAL\" if optimizer is None else \"TRAIN\", i, len(data), batch_time=avg_batch_time, loss=avg_loss,\n",
    "        #           top1=avg_top1_acc, top5=avg_top5_acc))\n",
    "            #if optimizer:\n",
    "                #loss_plot.plot()\n",
    "\n",
    "    # Print summary\n",
    "    #print('\\n===============> Total time {batch_time:d}s\\t'\n",
    "    #      'Avg loss {loss.avg:.4f}\\t'\n",
    "    #      'Avg Prec@1 {top1.avg:5.2f} %\\t'\n",
    "    #      'Avg Prec@5 {top5.avg:5.2f} %\\n'.format(\n",
    "    #       batch_time=int(avg_batch_time.sum), loss=avg_loss,\n",
    "    #       top1=avg_top1_acc, top5=avg_top5_acc))\n",
    "\n",
    "    return avg_top1_acc, avg_top5_acc, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "def main(batch_size=128, lr=0.1, epochs=5, cuda=False,  dropout_rate=0.1):\n",
    "\n",
    "    # ex :\n",
    "    #   {\"batch_size\": 128, \"epochs\": 5, \"lr\": 0.1}\n",
    "    \n",
    "    # define model, loss, optim\n",
    "    model = Simple3DCNN(dropout_prob=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr)\n",
    "\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "    if cuda: # only with GPU, and not with CPU\n",
    "        cudnn.benchmark = True\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    # Get the data\n",
    "    train, test, _ = get_loaders(batch_size, cuda)\n",
    "\n",
    "    # init plots\n",
    "    listm = []\n",
    "    #plot = AccLossPlot()\n",
    "    global loss_plot\n",
    "    loss_plot = TrainLossPlot()\n",
    "    #accs_train= []\n",
    "    #accs_test= []\n",
    "\n",
    "    # We iterate on the epochs\n",
    "    for i in range(epochs):\n",
    "        m = Metric()\n",
    "\n",
    "        # Train phase\n",
    "        top1_acc, avg_top5_acc, loss = epoch(train, model, criterion, optimizer, cuda)\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # Test phase\n",
    "        top1_acc_test, top5_acc_test, loss_test = epoch(test, model, criterion, cuda=cuda)\n",
    "        # plot\n",
    "        #plot.update(loss.avg, loss_test.avg, top1_acc.avg, top1_acc_test.avg)\n",
    "        m.acc_train = top1_acc.avg\n",
    "        m.acc_test = top1_acc_test.avg\n",
    "        m.loss_train = loss.avg\n",
    "        m.loss_test = loss_test.avg\n",
    "        listm.append(m)\n",
    "        print( f\"********** EPOCH {i+1} acc train={m.acc_train:.2f}%, acc test={m.acc_test:.2f}%, loss train={m.loss_train:.3f}, loss test={m.loss_test:.3f} **********\")\n",
    "        #accs_train.append(top1_acc_ = test)\n",
    "        #accs_test.append(top1_acc_test)\n",
    "\n",
    "    return listm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
